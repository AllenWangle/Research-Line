**DKN: Deep Knowledge-Aware Network for News Recommendation (WWW 2018)**
  - Hongwei Wang, Fuzheng Zhang, Xing Xie, Minyi Guo
  - [Paper](https://dl.acm.org/citation.cfm?id=3186175)
  - [tensorflow](https://github.com/hwwang55/DKN)

DKN is a content-based recomendation framework for click-through rate prediction. The key component of DKN is a multi-channel and word-entity-aligned knoledge-aware convolutional neural network (KCNN) that fuses semantic-level and knowledge-level representations of news. More specific, KCNN treats words and entities as multiple channels, and explicitly keep their alignment relationship during convolution. In addition, to address users' diverse interests, the model also uses an attention module to dynamically aggregate a user's history with respect to current candidate news.

**Problem defination**. The news recomendation is quite difficult as it poses three major challenges.
  1. news article are highly time-sensitive and their relevance expirs quickly within a short period.
  2. people are topic-sensitive in news reading as they usually intrested in multiple specific news categories. How to dynamically measure a user's interest based on his diversified reading history.
  3. News language is usually highly condensed and comprised of a lage amount of knowledge entities and commen sense.
So the problem is that given a set of user's news reading histories (only title) and a candidate news (only title), we need the model to predict the user would click this candidate news or not. So this paper treate this problem as a click-through rate prediction.

**Methodology**. This model is consist of three sub-module: **knowledge distillation**, **knowledge-aware CNN**, **attention-based user interest extraction**. The first sub-module need to be trained in preprocess. The rest sub-modules are trained then together in the influence of the first sub-module. In other words, the DKN model is a two-step model.

**knowledge distillation**. Associate the word in news with predefined entities in a knowledge graph. Based on these identified entities, we can construct a sub-graph and extract all relational links among them from the original knowledge graph. To address the problem that the entities may be sparse and lack diversity. Therefore we expand the knowledge sub-graph to all entities within one hop of identified ones (context entities). This process is as below:
$$context(e) = \left\{ e _ { i } | \left( e , r , e _ { i } \right) \in \mathcal { G } \text { or } \left( e _ { i } , r , e \right) \in G \right\}$$

$$\overline { \mathbf { e } } = \frac { 1 } { | \operatorname { contex } t ( e ) | } \sum _ { e _ { i } \in c o n t e x t ( e ) } \mathbf { e } _ { i }$$

**knowledge-aware CNN**. Given the learned entities embedding corresponding to the words(include the context entities), we can simply treat these entities as additional channels. Firstly, we transform the entity embedding to the word embedding space like below:

$$g \left( \mathbf { e } _ { 1 : n } \right) = \left[ g \left( \mathbf { e } _ { 1 } \right) g \left( \mathbf { e } _ { 2 } \right) \ldots g \left( \mathbf { e } _ { n } \right) \right]$$

$$g \left( \overline { \mathbf { e } } _ { 1 : n } \right) = \left[ g \left( \overline { \mathbf { e } } _ { 1 } \right) g \left( \overline { \mathbf { e } } _ { 2 } \right) \ldots g \left( \overline { \mathbf { e } } _ { n } \right) \right]$$

$$g ( \mathbf { e } ) = \mathbf { M } \mathbf { e }$$

$$g ( \mathbf { e } ) = \tanh ( \mathbf { M } \mathbf { e } + \mathbf { b } )$$

Then, we align and stack the three embedding matrices as:

$$\mathbf { W } = \left[ \left[ \mathbf { w } _ { 1 } g \left( \mathbf { e } _ { 1 } \right) g \left( \overline { \mathbf { e } } _ { 1 } \right) \right] \left[ \mathbf { w } _ { 2 } g \left( \mathbf { e } _ { 2 } \right) \overline { g } \left( \mathbf { e } _ { 2 } \right) \right] \ldots \left[ \mathbf { e } _ { n } g \left( \mathbf { e } _ { n } \right) g \left( \overline { \mathbf { e } } _ { n } \right) \right] \right] \in \mathbb { R } ^ { d \times n \times 3 }$$

Then we can do the CNN filters ($\boldsymbol { h } \in \mathbb { R } ^ { \tilde { d } \times l \times 3 }$) on the multi-channel embedding matrices:

$$c _ { i } ^ { h } = f \left( \boldsymbol { h } * \mathbf { W } _ { i : i + l - 1 } + b \right)$$

And use a max-over-time pooling operation on the output feature map and get the feature as below:

$$\overline { c } ^ { h } = \max \left\{ c _ { 1 } ^ { h } , c _ { 2 } ^ { h } , \ldots , c _ { n - l + 1 } ^ { h } \right\}$$

All feature (generated by different size filters) are concatenated and taken as the final representation $e(t)$ of the input news title $t$ as:

$$\mathbf { e } ( t ) = \left[ \tilde { c } ^ { h _ { 1 } } \tilde { c } ^ { h _ { 2 } } \ldots \tilde { c } ^ { h _ { m } } \right]$$.

**Attention-based User Interest Extraction**. a user’s interest in news topics may be various, and user $i$’s clicked items are supposed to have different impacts on the candidate news $t_{j}$ when considering whether user i will click $t_{j}$. We apply a DNN as the attention network:

$$s _ { t _ { k } ^ { i } , t _ { j } } = \operatorname { softmax } \left( \mathcal { H } \left( \mathbf { e } \left( t _ { k } ^ { i } \right) , \mathbf { e } \left( t _ { j } \right) \right) \right) = \frac { \exp \left( \mathcal { H } \left( \mathbf { e } \left( t _ { k } ^ { i } \right) , \mathbf { e } \left( t _ { j } \right) \right) \right) } { \sum _ { k = 1 } ^ { N _ { i } } \exp \left( \mathcal { H } \left( \mathbf { e } \left( t _ { k } ^ { i } \right) , \mathbf { e } \left( t _ { j } \right) \right) \right) }$$

Then we can use the weighted sum of the user's clicked new title embedding to represent him as below:

$$\mathbf { e } ( i ) = \sum _ { k = 1 } ^ { N _ { i } } s _ { t _ { k } ^ { i } , t _ { j } } \mathbf { e } \left( t _ { k } ^ { i } \right)$$

Finally the click is predicted by:
$$p _ { i , t _ { j } } = G \left( \mathbf { e } ( i ) , \mathbf { e } \left( t _ { j } \right) \right)$$

**Experiment**
The data analysis idea is good and we can borrow this
<p align="center">
  <img width="460" src="../assets/ReadMe-da2058ed.png">
</p>
And the finally main reuslt is as below:
<p align="center">
  <img width="460" src="../assets/ReadMe-dbfcea61.png">
</p>
